{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSI_Localization.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9NlD6xi-82yy",
        "aMkh0oTZ6pAE",
        "USmpaPgW-wbm",
        "w46sZUCxDRtz",
        "up7sKs8D_MB_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaheerr/Location_Detection/blob/main/CSI_Localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NlD6xi-82yy"
      },
      "source": [
        "# Loading GDrive and Install TF "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhHL-fDk89x2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "2075e328-03ff-495a-e95a-b2b605dfe3b6"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount ('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMhyGJuiVOW0"
      },
      "source": [
        "#! pip install tensorflow==1.13\n",
        "\n",
        "! python3 -c 'import tensorflow as tf; print(tf.__version__)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMkh0oTZ6pAE"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6APqdXhNQXi"
      },
      "source": [
        "#List Sorted Data in a Directory\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import csv\n",
        "\n",
        "for i in sorted(glob.glob(\"./drive/My Drive/Localization2/Dataset/Test/*\")):\n",
        "  print(\"input_file_name=\",i )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsizomNOl7L4"
      },
      "source": [
        "#Extract Rar file\n",
        "!unrar e './drive/My Drive/Localization2/t2.rar' './drive/My Drive/Localization2/' \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgLXJx_g6s6E"
      },
      "source": [
        "#Some values can become -inf which messes with the training\n",
        "#Cleaning Data (Romoving rows where the value in -inf ie, less than -10)\n",
        "#Do this for both test and Training Data (Change both i and outputfilename)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import csv\n",
        "\n",
        "for i in sorted(glob.glob(\"./drive/My Drive/Localization2/Dataset/Input/*\")):\n",
        "  print(\"input_file_name=\",i )\n",
        "  data = [[ float(elm) for elm in v] for v in csv.reader(open(i, \"r\"))]\n",
        "  tmp1 = np.array(data)\n",
        "  idx = np.where(tmp1 < -10)\n",
        "  print(idx)\n",
        "  tmp1 = np.delete(tmp1 , idx[0], axis = 0)\n",
        "  \n",
        "  outputfilename1 = i[0:39] + i[44:]\n",
        "  with open(outputfilename1, \"w\") as f:\n",
        "    print(outputfilename1)\n",
        "    writer = csv.writer(f, lineterminator=\"\\n\")\n",
        "    writer.writerows(tmp1)\n",
        "    \n",
        "  print(str(i) + \" -------->> done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCMPS5SVAQlX"
      },
      "source": [
        "#Check shape of Data (Rows and colums)\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import csv\n",
        "\n",
        "count = 0\n",
        "for i in sorted(glob.glob(\"./drive/My Drive/Localization2/Dataset/Input/*\")):\n",
        "  print(\"input_file_name=\",i ) \n",
        "  data = [[ float(elm) for elm in v] for v in csv.reader(open(i, \"r\"))]\n",
        "  tmp1 = np.array(data)\n",
        "  print(tmp1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXKThm8H8-ZB"
      },
      "source": [
        "# Loading Data from Drive, divinding into test, train, val and Normalizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLgnFgLH9F82"
      },
      "source": [
        "#Importing Training Data for classification\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import csv\n",
        "\n",
        "ds = 50 #Downscaling factor\n",
        "x = np.empty(shape = [0,90], dtype = float)\n",
        "y = np.empty(shape = [0,15], dtype = float)\n",
        "count = 0\n",
        "persons = 3 #Number of Persons\n",
        "\n",
        "p = 0;\n",
        "for i in sorted(glob.glob(\"./drive/My Drive/Localization2/Dataset/Input/*\")):\n",
        "  print(\"input_file_name=\",i ) \n",
        "  data = [[ float(elm) for elm in v] for v in csv.reader(open(i, \"r\"))]\n",
        "  tmp1 = np.array(data)\n",
        "  tmp1 = tmp1[:59200,1:91]\n",
        "  tmp2 = tmp1[::ds,:]\n",
        "  #r,c = np.shape(tmp1)\n",
        "  x = np.concatenate((x,tmp2),axis = 0)\n",
        "  r = tmp2.shape[0]\n",
        "  \n",
        "  yy = np.empty([r,15],float)\n",
        "  if count == 0 :\n",
        "      yy[:,:] = np.array([1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
        "      print('Dome')\n",
        "  elif count == 1:\n",
        "     yy[:,:] = np.array([0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
        "  elif count == 2:\n",
        "     yy[:,:] = np.array([0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
        "  elif count == 3:\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
        "  elif count == 4:\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
        "  elif count == 5:\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
        "  elif count == 6:\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
        "  elif count == 7:\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
        "  elif count == 8:\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
        "  elif count == 9:\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])\n",
        "  elif count == 10:\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])\n",
        "  elif count == 11:\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0])\n",
        "  elif count == 12:\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0])\n",
        "  elif count == 13:\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0])\n",
        "  elif count == 14:\n",
        "     yy[:,:] = np.array([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0])\n",
        "\n",
        "\n",
        "  y = np.concatenate((y,yy),axis = 0)\n",
        "  p = (p+1)%persons\n",
        "  if p == 0:\n",
        "    count = count + 1\n",
        "  \n",
        "  print(np.shape(x))\n",
        "  print(np.shape(y))\n",
        "  print(str(i) + \" -------->> done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz1pADSp9PYg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3137
        },
        "outputId": "82e43cd2-429e-4144-b04d-39daf8a38491"
      },
      "source": [
        "#Importing Training Data for Regression\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import csv\n",
        "\n",
        "ds = 50 #Downsampling Factor\n",
        "x = np.empty(shape = [0,90], dtype = float)\n",
        "y = np.empty(shape = [0,2], dtype = float)\n",
        "count = 0\n",
        "persons = 3 #Number of Persons\n",
        "\n",
        "p = 0;\n",
        "for i in sorted(glob.glob(\"./drive/My Drive/Localization2/Dataset/Input/*\")):\n",
        "  print(\"input_file_name=\",i ) \n",
        "  data = [[ float(elm) for elm in v] for v in csv.reader(open(i, \"r\"))]\n",
        "  tmp1 = np.array(data)\n",
        "  tmp1 = tmp1[:59200,1:91]\n",
        "  tmp2 = tmp1[::ds,:]\n",
        "  #r,c = np.shape(tmp1)\n",
        "  x = np.concatenate((x,tmp2),axis = 0)\n",
        "  r = tmp2.shape[0]\n",
        "  \n",
        "  yy = np.empty([r,2],float)\n",
        "  if count == 0 :\n",
        "     yy[:,:] = np.array([0.0,0.0])\n",
        "  elif count == 1:\n",
        "     yy[:,:] = np.array([0.0,0.9])\n",
        "  elif count == 2:\n",
        "     yy[:,:] = np.array([0.0,1.8])\n",
        "  elif count == 3:\n",
        "     yy[:,:] = np.array([0.9,0.0])\n",
        "  elif count == 4:\n",
        "     yy[:,:] = np.array([0.9,0.9])\n",
        "  elif count == 5:\n",
        "     yy[:,:] = np.array([0.9,1.8])\n",
        "  elif count == 6:\n",
        "     yy[:,:] = np.array([1.8,0.0])\n",
        "  elif count == 7:\n",
        "     yy[:,:] = np.array([1.8,0.9])\n",
        "  elif count == 8:\n",
        "     yy[:,:] = np.array([1.8,1.8])\n",
        "  elif count == 9:\n",
        "     yy[:,:] = np.array([2.7,0.0])\n",
        "  elif count == 10:\n",
        "     yy[:,:] = np.array([2.7,0.9])\n",
        "  elif count == 11:\n",
        "     yy[:,:] = np.array([2.7,1.8])\n",
        "  elif count == 12:\n",
        "     yy[:,:] = np.array([3.6,0.0])\n",
        "  elif count == 13:\n",
        "     yy[:,:] = np.array([3.6,0.9])\n",
        "  elif count == 14:\n",
        "     yy[:,:] = np.array([3.6,1.8])\n",
        "\n",
        "\n",
        "  y = np.concatenate((y,yy),axis = 0)\n",
        "  p = (p+1)%persons\n",
        "  if p == 0:\n",
        "    count = count + 1  \n",
        "  print(np.shape(x))\n",
        "  print(np.shape(y))\n",
        "  print(str(i) + \" -------->> done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc111.dat.csv\n",
            "(1184, 90)\n",
            "(1184, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc111.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc112.dat.csv\n",
            "(2368, 90)\n",
            "(2368, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc112.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc113.dat.csv\n",
            "(3552, 90)\n",
            "(3552, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc113.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc121.dat.csv\n",
            "(4736, 90)\n",
            "(4736, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc121.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc122.dat.csv\n",
            "(5920, 90)\n",
            "(5920, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc122.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc123.dat.csv\n",
            "(7104, 90)\n",
            "(7104, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc123.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc131.dat.csv\n",
            "(8288, 90)\n",
            "(8288, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc131.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc132.dat.csv\n",
            "(9472, 90)\n",
            "(9472, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc132.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc133.dat.csv\n",
            "(10656, 90)\n",
            "(10656, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc133.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc211.dat.csv\n",
            "(11840, 90)\n",
            "(11840, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc211.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc212.dat.csv\n",
            "(13024, 90)\n",
            "(13024, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc212.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc213.dat.csv\n",
            "(14208, 90)\n",
            "(14208, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc213.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc221.dat.csv\n",
            "(15392, 90)\n",
            "(15392, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc221.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc222.dat.csv\n",
            "(16576, 90)\n",
            "(16576, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc222.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc223.dat.csv\n",
            "(17760, 90)\n",
            "(17760, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc223.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc231.dat.csv\n",
            "(18944, 90)\n",
            "(18944, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc231.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc232.dat.csv\n",
            "(20128, 90)\n",
            "(20128, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc232.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc233.dat.csv\n",
            "(21312, 90)\n",
            "(21312, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc233.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc311.dat.csv\n",
            "(22496, 90)\n",
            "(22496, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc311.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc312.dat.csv\n",
            "(23680, 90)\n",
            "(23680, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc312.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc313.dat.csv\n",
            "(24864, 90)\n",
            "(24864, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc313.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc321.dat.csv\n",
            "(26048, 90)\n",
            "(26048, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc321.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc322.dat.csv\n",
            "(27232, 90)\n",
            "(27232, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc322.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc323.dat.csv\n",
            "(28416, 90)\n",
            "(28416, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc323.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc331.dat.csv\n",
            "(29600, 90)\n",
            "(29600, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc331.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc332.dat.csv\n",
            "(30784, 90)\n",
            "(30784, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc332.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc333.dat.csv\n",
            "(31968, 90)\n",
            "(31968, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc333.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc411.dat.csv\n",
            "(33152, 90)\n",
            "(33152, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc411.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc412.dat.csv\n",
            "(34336, 90)\n",
            "(34336, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc412.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc413.dat.csv\n",
            "(35520, 90)\n",
            "(35520, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc413.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc421.dat.csv\n",
            "(36704, 90)\n",
            "(36704, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc421.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc422.dat.csv\n",
            "(37888, 90)\n",
            "(37888, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc422.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc423.dat.csv\n",
            "(39072, 90)\n",
            "(39072, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc423.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc431.dat.csv\n",
            "(40256, 90)\n",
            "(40256, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc431.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc432.dat.csv\n",
            "(41440, 90)\n",
            "(41440, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc432.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc433.dat.csv\n",
            "(42624, 90)\n",
            "(42624, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc433.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc511.dat.csv\n",
            "(43808, 90)\n",
            "(43808, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc511.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc512.dat.csv\n",
            "(44992, 90)\n",
            "(44992, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc512.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc513.dat.csv\n",
            "(46176, 90)\n",
            "(46176, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc513.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc521.dat.csv\n",
            "(47360, 90)\n",
            "(47360, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc521.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc522.dat.csv\n",
            "(48544, 90)\n",
            "(48544, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc522.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc523.dat.csv\n",
            "(49728, 90)\n",
            "(49728, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc523.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc531.dat.csv\n",
            "(50912, 90)\n",
            "(50912, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc531.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc532.dat.csv\n",
            "(52096, 90)\n",
            "(52096, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc532.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Input/csi_loc533.dat.csv\n",
            "(53280, 90)\n",
            "(53280, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Input/csi_loc533.dat.csv -------->> done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btH26huVykVd"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "loc = 9\n",
        "n= 1184\n",
        "plt.subplot(311)\n",
        "plt.plot(x[loc*n:loc*n+10,:].T)\n",
        "plt.subplot(312)\n",
        "plt.plot(x[loc*2*n:loc*2*n+10,:].T)\n",
        "plt.subplot(313)\n",
        "plt.plot(x[loc*3*n:loc*3*n+10,:].T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1D8QKdu-Cmy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "0122502f-2146-4126-97ac-cf22c5f345bc"
      },
      "source": [
        "#Importing Testing Data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import csv\n",
        "\n",
        "ds = 50 #Downsampling Factor\n",
        "persons = 2\n",
        "x_test1 = np.empty(shape = [0,90], dtype = float)\n",
        "y_test1 = np.empty(shape = [0,2], dtype = float)\n",
        "\n",
        "count = 0\n",
        "p = 0\n",
        "for i in sorted(glob.glob(\"./drive/My Drive/Localization2/Dataset/Test/*\")):\n",
        "  print(\"input_file_name=\",i )\n",
        "  data = [[ float(elm) for elm in v] for v in csv.reader(open(i, \"r\"))]\n",
        "  tmp1 = np.array(data)\n",
        "  tmp1 = tmp1[:59200,1:91]\n",
        "  tmp2 = tmp1[::ds,:]\n",
        "  #r,c = np.shape(tmp1)\n",
        "  x_test1 = np.concatenate((x_test1,tmp2),axis = 0)\n",
        "  r = tmp2.shape[0]\n",
        "  yy = np.empty([r,2],float)\n",
        "  \n",
        "  \n",
        "  if count == 0 :\n",
        "      yy[:,:] = np.array([1.2,0.9])\n",
        "      print('done')\n",
        "  elif count == 1:\n",
        "     yy[:,:] = np.array([1.8,1.5])\n",
        "  elif count == 2:\n",
        "     yy[:,:] = np.array([2.4,0.9])\n",
        "  elif count == 3:\n",
        "     yy[:,:] = np.array([1.8,0.3])\n",
        "      \n",
        "  y_test1 = np.concatenate((y_test1,yy),axis = 0)\n",
        "  \n",
        "  p = (p+1)%persons\n",
        "  if p == 0:\n",
        "    count = count + 1\n",
        "  \n",
        "  print(np.shape(x_test1))\n",
        "  print(np.shape(y_test1))\n",
        "  print(str(i) + \" -------->> done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Test/test11.dat.csv\n",
            "done\n",
            "(1184, 90)\n",
            "(1184, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Test/test11.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Test/test12.dat.csv\n",
            "done\n",
            "(2368, 90)\n",
            "(2368, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Test/test12.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Test/test21.dat.csv\n",
            "(3552, 90)\n",
            "(3552, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Test/test21.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Test/test22.dat.csv\n",
            "(4736, 90)\n",
            "(4736, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Test/test22.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Test/test31.dat.csv\n",
            "(5920, 90)\n",
            "(5920, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Test/test31.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Test/test32.dat.csv\n",
            "(7104, 90)\n",
            "(7104, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Test/test32.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Test/test41.dat.csv\n",
            "(8288, 90)\n",
            "(8288, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Test/test41.dat.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Test/test42.dat.csv\n",
            "(9472, 90)\n",
            "(9472, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Test/test42.dat.csv -------->> done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA15R0se-L3z"
      },
      "source": [
        "#Divide Data into Train , validation and Test Sets.\n",
        "perm = np.arange(x.shape[0])\n",
        "np.random.shuffle(perm)\n",
        "x = x[perm]\n",
        "y = y[perm]\n",
        "\n",
        "train_idx = int(np.ceil(0.8 * x.shape[0]))\n",
        "val_idx = int(np.ceil(0.9 * x.shape[0]))\n",
        "\n",
        "x_train, x_val, x_test = x[:train_idx, ...], x[train_idx:val_idx:, ...], x[val_idx:, ...]\n",
        "y_train, y_val, y_test = y[:train_idx, ...], y[train_idx:val_idx:, ...], y[val_idx:, ...]\n",
        "\n",
        "#Save perm from drive (In case if training stops and you want to recover test and training split from drive)\n",
        "\n",
        "outputfilename1 = \"./drive/My Drive/Localization2/perm/perm.csv\"\n",
        "with open(outputfilename1, \"w\") as f:\n",
        "  writer = csv.writer(f, lineterminator=\"\\n\")\n",
        "  writer.writerows(map(lambda n: [n], perm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzaUoi9e-ZlG"
      },
      "source": [
        "#Load perm from drive\n",
        "f = \"./drive/My Drive/Localization2/perm/perm.csv\"\n",
        "print(\"input_file_name=\",f)\n",
        "perm = np.array([[ int(elm) for elm in v] for v in csv.reader(open(f, \"r\"))])\n",
        "perm = perm[:]\n",
        "x = x[perm]\n",
        "y = y[perm]\n",
        "\n",
        "train_idx = int(np.ceil(0.8 * x.shape[0]))\n",
        "val_idx = int(np.ceil(0.9 * x.shape[0]))\n",
        "\n",
        "x_train, x_val, x_test = np.squeeze(x[:train_idx, ...]), np.squeeze(x[train_idx:val_idx:, ...]), np.squeeze(x[val_idx:, ...])\n",
        "y_train, y_val, y_test = np.squeeze(y[:train_idx, ...]), np.squeeze(y[train_idx:val_idx:, ...]), np.squeeze(y[val_idx:, ...])\n",
        "\n",
        "x_train = np.squeeze(x_train)\n",
        "x_val = np.squeeze(x_val)\n",
        "x_test = np.squeeze(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzK7t-7--cRv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bdbfe4e6-7043-4725-a7dc-f9d6e6882d24"
      },
      "source": [
        "#Normalizing Data from 0 to 1 range\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "scaler.fit(x_train) \n",
        "\n",
        "x_train = scaler.transform(x_train)\n",
        "x_val = scaler.transform(x_val)\n",
        "x_test = scaler.transform(x_test)\n",
        "x_test1 = scaler.transform(x_test1)\n",
        "print('Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG7xVFX5-fsc"
      },
      "source": [
        "# Pre-training using AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OcoBDXZ-jr_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "721a3ec1-c9eb-4876-b8ae-06e09edee8ab"
      },
      "source": [
        "%env KERAS_BACKEND=theano\n",
        "#%reset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.layers import Input, Convolution2D, Activation, MaxPooling2D, \\\n",
        "     Dense, BatchNormalization, Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.optimizers import SGD\n",
        "from keras.models import Model\n",
        "from keras.utils import np_utils\n",
        "from keras.constraints import maxnorm\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "print(keras.__version__)\n",
        "from subprocess import check_output\n",
        "\n",
        "# Layer by layer pretraining Models\n",
        "\n",
        "n1 = 120\n",
        "n2 = 110\n",
        "n3 = 100\n",
        "\n",
        "# Layer 1\n",
        "input_img = Input(shape = (90, ))\n",
        "distorted_input1 = Dropout(.1)(input_img)\n",
        "encoded1 = Dense(n1, activation = 'relu')(distorted_input1)\n",
        "encoded1_bn = BatchNormalization()(encoded1)\n",
        "decoded1 = Dense(90, activation = 'relu')(encoded1_bn)\n",
        "\n",
        "autoencoder1 = Model(input = input_img, output = decoded1)\n",
        "encoder1 = Model(input = input_img, output = encoded1_bn)\n",
        "\n",
        "# Layer 2\n",
        "encoded1_input = Input(shape = (n1,))\n",
        "distorted_input2 = Dropout(.2)(encoded1_input)\n",
        "encoded2 = Dense(n2, activation = 'relu')(distorted_input2)\n",
        "encoded2_bn = BatchNormalization()(encoded2)\n",
        "decoded2 = Dense(n1, activation = 'relu')(encoded2_bn)\n",
        "\n",
        "autoencoder2 = Model(input = encoded1_input, output = decoded2)\n",
        "encoder2 = Model(input = encoded1_input, output = encoded2_bn)\n",
        "\n",
        "# Layer 3 \n",
        "encoded2_input = Input(shape = (n2,))\n",
        "distorted_input3 = Dropout(.3)(encoded2_input)\n",
        "encoded3 = Dense(n3, activation = 'relu')(distorted_input3)\n",
        "encoded3_bn = BatchNormalization()(encoded3)\n",
        "decoded3 = Dense(n2, activation = 'relu')(encoded3_bn)\n",
        "\n",
        "autoencoder3 = Model(input = encoded2_input, output = decoded3)\n",
        "encoder3 = Model(input = encoded2_input, output = encoded3_bn)\n",
        "\n",
        "sgd1 = SGD(lr = 5, decay = 0.5, momentum = .85, nesterov = True)\n",
        "sgd2 = SGD(lr = 5, decay = 0.5, momentum = .85, nesterov = True)\n",
        "sgd3 = SGD(lr = 0.5, decay = 0.5, momentum = .85, nesterov = True)\n",
        "\n",
        "autoencoder1.compile(loss='mse', optimizer = sgd1)\n",
        "autoencoder2.compile(loss='mse', optimizer = sgd2)\n",
        "autoencoder3.compile(loss='mse', optimizer = sgd3)\n",
        "\n",
        "encoder1.compile(loss='mse', optimizer = sgd1)\n",
        "encoder2.compile(loss='mse', optimizer = sgd1)\n",
        "encoder3.compile(loss='mse', optimizer = sgd1)\n",
        "\n",
        "\n",
        "# What will happen to the learnning rates under this decay schedule?\n",
        "lr = 5\n",
        "for i in range(12):\n",
        "    lr = lr - lr * .15\n",
        "    print(lr)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: KERAS_BACKEND=theano\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using Theano backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.2.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=/input_1, outputs=Elemwise{m...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=/input_1, outputs=if{}.0)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=/input_2, outputs=Elemwise{m...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=/input_2, outputs=if{}.0)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4.25\n",
            "3.6125\n",
            "3.0706249999999997\n",
            "2.6100312499999996\n",
            "2.2185265624999997\n",
            "1.8857475781249997\n",
            "1.60288544140625\n",
            "1.3624526251953124\n",
            "1.1580847314160156\n",
            "0.9843720217036133\n",
            "0.8367162184480713\n",
            "0.7112087856808607\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:56: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=/input_3, outputs=Elemwise{m...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:57: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=/input_3, outputs=if{}.0)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzSbFJTe-l3V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3570
        },
        "outputId": "b7f49286-65e5-45c0-f4aa-2ba785b7ff5a"
      },
      "source": [
        "autoencoder1.fit(x_train, x_train,\n",
        "                nb_epoch = 100, batch_size = 1024,\n",
        "                validation_split = 0.10,\n",
        "                shuffle = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  after removing the cwd from sys.path.\n",
            "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 38361 samples, validate on 4263 samples\n",
            "Epoch 1/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 0.0965 - val_loss: 0.7165\n",
            "Epoch 2/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 0.0168 - val_loss: 0.1444\n",
            "Epoch 3/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0095 - val_loss: 0.0544\n",
            "Epoch 4/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0075 - val_loss: 0.0247\n",
            "Epoch 5/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0066 - val_loss: 0.0123\n",
            "Epoch 6/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0064 - val_loss: 0.0071\n",
            "Epoch 7/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0063 - val_loss: 0.0046\n",
            "Epoch 8/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0062 - val_loss: 0.0035\n",
            "Epoch 9/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 0.0061 - val_loss: 0.0029\n",
            "Epoch 10/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0060 - val_loss: 0.0026\n",
            "Epoch 11/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0060 - val_loss: 0.0025\n",
            "Epoch 12/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0059 - val_loss: 0.0024\n",
            "Epoch 13/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0058 - val_loss: 0.0023\n",
            "Epoch 14/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0058 - val_loss: 0.0023\n",
            "Epoch 15/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0058 - val_loss: 0.0023\n",
            "Epoch 16/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0057 - val_loss: 0.0023\n",
            "Epoch 17/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0057 - val_loss: 0.0023\n",
            "Epoch 18/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0057 - val_loss: 0.0023\n",
            "Epoch 19/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0056 - val_loss: 0.0023\n",
            "Epoch 20/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0056 - val_loss: 0.0022\n",
            "Epoch 21/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0056 - val_loss: 0.0022\n",
            "Epoch 22/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0056 - val_loss: 0.0022\n",
            "Epoch 23/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0056 - val_loss: 0.0022\n",
            "Epoch 24/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0055 - val_loss: 0.0022\n",
            "Epoch 25/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0055 - val_loss: 0.0022\n",
            "Epoch 26/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0055 - val_loss: 0.0022\n",
            "Epoch 27/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0055 - val_loss: 0.0022\n",
            "Epoch 28/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0055 - val_loss: 0.0022\n",
            "Epoch 29/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0055 - val_loss: 0.0022\n",
            "Epoch 30/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0055 - val_loss: 0.0022\n",
            "Epoch 31/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0055 - val_loss: 0.0022\n",
            "Epoch 32/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0054 - val_loss: 0.0022\n",
            "Epoch 33/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 0.0054 - val_loss: 0.0022\n",
            "Epoch 34/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0054 - val_loss: 0.0022\n",
            "Epoch 35/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0054 - val_loss: 0.0022\n",
            "Epoch 36/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0054 - val_loss: 0.0022\n",
            "Epoch 37/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0054 - val_loss: 0.0022\n",
            "Epoch 38/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0054 - val_loss: 0.0022\n",
            "Epoch 39/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0054 - val_loss: 0.0021\n",
            "Epoch 40/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 41/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0054 - val_loss: 0.0021\n",
            "Epoch 42/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 0.0054 - val_loss: 0.0021\n",
            "Epoch 43/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 44/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 45/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 46/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 47/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 48/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 49/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 50/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 51/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 52/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 53/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 54/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 55/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 56/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 57/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 58/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 59/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 60/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 61/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 62/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 63/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 64/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 65/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 66/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 67/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 68/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 69/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 70/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 71/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 72/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 73/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 74/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 75/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 76/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 77/100\n",
            "38361/38361 [==============================] - 1s 25us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 78/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 79/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 80/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 81/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 82/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 83/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 84/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 85/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 86/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 87/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 88/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 89/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 90/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 91/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 92/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 93/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 94/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 95/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 96/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 97/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 98/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0051 - val_loss: 0.0020\n",
            "Epoch 99/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 0.0051 - val_loss: 0.0020\n",
            "Epoch 100/100\n",
            "38361/38361 [==============================] - 1s 22us/step - loss: 0.0051 - val_loss: 0.0020\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1082258f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT2ixEj9-ngn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aef4e8b4-ac43-4ca2-9674-83619eadc102"
      },
      "source": [
        "first_layer_code = encoder1.predict(x_train)\n",
        "print(first_layer_code.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42624, 120)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb7Z9S5U-pRZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3553
        },
        "outputId": "c12a88ad-988a-4bc3-ed07-029b23b3998d"
      },
      "source": [
        "autoencoder2.fit(first_layer_code, first_layer_code,\n",
        "                nb_epoch = 100, batch_size = 512,\n",
        "                validation_split = 0.15,\n",
        "                shuffle = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 36230 samples, validate on 6394 samples\n",
            "Epoch 1/100\n",
            "36230/36230 [==============================] - 1s 26us/step - loss: 0.2572 - val_loss: 0.2476\n",
            "Epoch 2/100\n",
            "36230/36230 [==============================] - 1s 26us/step - loss: 0.2084 - val_loss: 0.2102\n",
            "Epoch 3/100\n",
            "36230/36230 [==============================] - 1s 25us/step - loss: 0.2083 - val_loss: 0.2088\n",
            "Epoch 4/100\n",
            "36230/36230 [==============================] - 1s 25us/step - loss: 0.2080 - val_loss: 0.2084\n",
            "Epoch 5/100\n",
            "36230/36230 [==============================] - 1s 25us/step - loss: 0.2075 - val_loss: 0.2076\n",
            "Epoch 6/100\n",
            "36230/36230 [==============================] - 1s 26us/step - loss: 0.2066 - val_loss: 0.2067\n",
            "Epoch 7/100\n",
            "36230/36230 [==============================] - 1s 26us/step - loss: 0.2057 - val_loss: 0.2061\n",
            "Epoch 8/100\n",
            "36230/36230 [==============================] - 1s 29us/step - loss: 0.2054 - val_loss: 0.2061\n",
            "Epoch 9/100\n",
            "36230/36230 [==============================] - 1s 29us/step - loss: 0.2054 - val_loss: 0.2061\n",
            "Epoch 10/100\n",
            "36230/36230 [==============================] - 1s 29us/step - loss: 0.2054 - val_loss: 0.2061\n",
            "Epoch 11/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2061\n",
            "Epoch 12/100\n",
            "36230/36230 [==============================] - 1s 31us/step - loss: 0.2054 - val_loss: 0.2061\n",
            "Epoch 13/100\n",
            "36230/36230 [==============================] - 1s 31us/step - loss: 0.2054 - val_loss: 0.2061\n",
            "Epoch 14/100\n",
            "36230/36230 [==============================] - 1s 31us/step - loss: 0.2054 - val_loss: 0.2061\n",
            "Epoch 15/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2061\n",
            "Epoch 16/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2061\n",
            "Epoch 17/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2061\n",
            "Epoch 18/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 19/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 20/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 21/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 22/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 23/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 24/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 25/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 26/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 27/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 28/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 29/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 30/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 31/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 32/100\n",
            "36230/36230 [==============================] - 1s 29us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 33/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 34/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 35/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 36/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 37/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 38/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 39/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 40/100\n",
            "36230/36230 [==============================] - 1s 29us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 41/100\n",
            "36230/36230 [==============================] - 1s 29us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 42/100\n",
            "36230/36230 [==============================] - 1s 29us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 43/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 44/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 45/100\n",
            "36230/36230 [==============================] - 1s 31us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 46/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 47/100\n",
            "36230/36230 [==============================] - 1s 31us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 48/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 49/100\n",
            "36230/36230 [==============================] - 1s 29us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 50/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 51/100\n",
            "36230/36230 [==============================] - 1s 29us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 52/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 53/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 54/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 55/100\n",
            "36230/36230 [==============================] - 1s 29us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 56/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 57/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 58/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 59/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 60/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 61/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 62/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 63/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 64/100\n",
            "36230/36230 [==============================] - 1s 29us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 65/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2054 - val_loss: 0.2060\n",
            "Epoch 66/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2053 - val_loss: 0.2059\n",
            "Epoch 67/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 68/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 69/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 70/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 71/100\n",
            "36230/36230 [==============================] - 1s 31us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 72/100\n",
            "36230/36230 [==============================] - 1s 31us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 73/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 74/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 75/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 76/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 77/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 78/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 79/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 80/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 81/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 82/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 83/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 84/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 85/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 86/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 87/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 88/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 89/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 90/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 91/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 92/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 93/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 94/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 95/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 96/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 97/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 98/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 99/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n",
            "Epoch 100/100\n",
            "36230/36230 [==============================] - 1s 30us/step - loss: 0.2052 - val_loss: 0.2059\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f106edee320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypOnlpyq-qhq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "422aac5a-dbf1-42c6-b955-f25017daccbb"
      },
      "source": [
        "second_layer_code = encoder2.predict(first_layer_code)\n",
        "print(second_layer_code.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42624, 110)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvrrm4Hm-rrK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1421
        },
        "outputId": "b8434ca5-5ae2-47a4-ebf6-e07105012886"
      },
      "source": [
        "autoencoder3.fit(second_layer_code, second_layer_code,\n",
        "                nb_epoch = 100, batch_size = 512,\n",
        "                validation_split = 0.10,\n",
        "                shuffle = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 38361 samples, validate on 4263 samples\n",
            "Epoch 1/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.8405 - val_loss: 1.7735\n",
            "Epoch 2/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.6278 - val_loss: 1.5533\n",
            "Epoch 3/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.5784 - val_loss: 1.4898\n",
            "Epoch 4/100\n",
            "38361/38361 [==============================] - 1s 25us/step - loss: 1.5518 - val_loss: 1.4628\n",
            "Epoch 5/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.5330 - val_loss: 1.4480\n",
            "Epoch 6/100\n",
            "38361/38361 [==============================] - 1s 23us/step - loss: 1.5182 - val_loss: 1.4372\n",
            "Epoch 7/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.5078 - val_loss: 1.4284\n",
            "Epoch 8/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4983 - val_loss: 1.4214\n",
            "Epoch 9/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4905 - val_loss: 1.4154\n",
            "Epoch 10/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4831 - val_loss: 1.4107\n",
            "Epoch 11/100\n",
            "38361/38361 [==============================] - 1s 25us/step - loss: 1.4775 - val_loss: 1.4063\n",
            "Epoch 12/100\n",
            "38361/38361 [==============================] - 1s 25us/step - loss: 1.4719 - val_loss: 1.4026\n",
            "Epoch 13/100\n",
            "38361/38361 [==============================] - 1s 25us/step - loss: 1.4682 - val_loss: 1.3993\n",
            "Epoch 14/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4641 - val_loss: 1.3963\n",
            "Epoch 15/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4608 - val_loss: 1.3936\n",
            "Epoch 16/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4569 - val_loss: 1.3913\n",
            "Epoch 17/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4546 - val_loss: 1.3891\n",
            "Epoch 18/100\n",
            "38361/38361 [==============================] - 1s 25us/step - loss: 1.4513 - val_loss: 1.3873\n",
            "Epoch 19/100\n",
            "38361/38361 [==============================] - 1s 25us/step - loss: 1.4491 - val_loss: 1.3854\n",
            "Epoch 20/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4470 - val_loss: 1.3836\n",
            "Epoch 21/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4452 - val_loss: 1.3821\n",
            "Epoch 22/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4422 - val_loss: 1.3807\n",
            "Epoch 23/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4416 - val_loss: 1.3795\n",
            "Epoch 24/100\n",
            "38361/38361 [==============================] - 1s 25us/step - loss: 1.4394 - val_loss: 1.3783\n",
            "Epoch 25/100\n",
            "38361/38361 [==============================] - 1s 25us/step - loss: 1.4379 - val_loss: 1.3773\n",
            "Epoch 26/100\n",
            "38361/38361 [==============================] - 1s 25us/step - loss: 1.4369 - val_loss: 1.3761\n",
            "Epoch 27/100\n",
            "38361/38361 [==============================] - 1s 25us/step - loss: 1.4352 - val_loss: 1.3751\n",
            "Epoch 28/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4333 - val_loss: 1.3741\n",
            "Epoch 29/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4324 - val_loss: 1.3732\n",
            "Epoch 30/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4313 - val_loss: 1.3722\n",
            "Epoch 31/100\n",
            "38361/38361 [==============================] - 1s 25us/step - loss: 1.4299 - val_loss: 1.3714\n",
            "Epoch 32/100\n",
            "38361/38361 [==============================] - 1s 25us/step - loss: 1.4296 - val_loss: 1.3707\n",
            "Epoch 33/100\n",
            "38361/38361 [==============================] - 1s 26us/step - loss: 1.4292 - val_loss: 1.3699\n",
            "Epoch 34/100\n",
            "38361/38361 [==============================] - 2s 51us/step - loss: 1.4276 - val_loss: 1.3693\n",
            "Epoch 35/100\n",
            "38361/38361 [==============================] - 1s 32us/step - loss: 1.4273 - val_loss: 1.3687\n",
            "Epoch 36/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4257 - val_loss: 1.3682\n",
            "Epoch 37/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4246 - val_loss: 1.3674\n",
            "Epoch 38/100\n",
            "38361/38361 [==============================] - 1s 24us/step - loss: 1.4239 - val_loss: 1.3668\n",
            "Epoch 39/100\n",
            "35840/38361 [===========================>..] - ETA: 0s - loss: 1.4233"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8wDIDaVXfub"
      },
      "source": [
        "#Save autoencoder weights after training\n",
        "autoencoder1.save('./drive/My Drive/Localization2/keras_models/15-loc-1/a1-10hz-2p.h5')\n",
        "autoencoder2.save('./drive/My Drive/Localization2/keras_models/15-loc-1/a2-10hz-2p.h5')\n",
        "autoencoder3.save('./drive/My Drive/Localization2/keras_models/15-loc-1/a3-10hz-2p.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbeHOOMrX_mK"
      },
      "source": [
        "#Load autoencoder weights\n",
        "autoencoder1.load_weights('./drive/My Drive/Localization2/keras_models/15-loc-1/a1-10hz-2p.h5')\n",
        "autoencoder2.load_weights('./drive/My Drive/Localization2/keras_models/15-loc-1/a2-10hz-2p.h5')\n",
        "autoencoder3.load_weights('./drive/My Drive/Localization2/keras_models/15-loc-1/a3-10hz-2p.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USmpaPgW-wbm"
      },
      "source": [
        "# Defining Models for Classification and Regression (with AutoEncoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQqBIqxO-1dr"
      },
      "source": [
        "#Machine Learning Model for classification\n",
        "#Plot Loss\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Flatten, Dense, Activation\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM,Dropout,Conv2D\n",
        "from tensorflow.python.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.layers import BatchNormalization\n",
        "from tensorflow.python.keras.layers import Flatten,MaxPooling2D,AveragePooling2D\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "\n",
        "class PlotLosses(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        \n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.i += 1\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        plt.plot(self.x, self.losses, label=\"loss\")\n",
        "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        plt.legend()\n",
        "        plt.show();\n",
        "        \n",
        "plot_losses = PlotLosses()\n",
        "\n",
        "\n",
        "class PlotLearning(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.acc = []\n",
        "        self.val_acc = []\n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "       \n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.acc.append(logs.get('acc'))\n",
        "        self.val_acc.append(logs.get('val_acc'))\n",
        "        self.i += 1\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        ax1.set_yscale('log')\n",
        "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
        "        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        ax1.legend()\n",
        "        \n",
        "        ax2.plot(self.x, self.acc, label=\"accuracy\")\n",
        "        ax2.plot(self.x, self.val_acc, label=\"validation accuracy\")\n",
        "        ax2.legend()\n",
        "        \n",
        "        plt.show();\n",
        "        \n",
        "plot = PlotLearning()\n",
        "\n",
        "\n",
        "\n",
        "class AvgError(keras.callbacks.Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "      {}\n",
        "      \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        x_test, y_test = self.test_data\n",
        "        #loss, acc = self.model.evaluate(x, y, verbose=0)\n",
        "        k = 24\n",
        "        pred = np.array(self.model.predict(x_test,verbose = 0))\n",
        "        top_k_idx = pred.argsort(axis = 1)[:,-1*k:][:,::-1]\n",
        "        top_k_preds = np.empty([pred.shape[0],k])\n",
        "\n",
        "        for k in range(pred.shape[0]):\n",
        "          top_k_preds[k] = pred[k,:][top_k_idx[k,:]]\n",
        "\n",
        "\n",
        "        cord = np.array([[0.0,0.0],[0.0,0.6],[0.0,1.2],[0.0,1.8],[0.0,2.4],[0.0,3.0],[0.0,3.6],[0.0,4.2],[0.6,0.0],[0.6,0.6],[0.6,1.2],[0.6,1.8],[0.6,2.4],[0.6,3.0],[0.6,3.6],[0.6,4.2],\n",
        "                        [1.2,0.0],[1.2,0.6],[1.2,1.2],[1.2,1.8],[1.2,2.4],[1.2,3.0],[1.2,3.6],[1.2,4.2]])\n",
        "        \n",
        "#        cord = np.array([[0.334,0.0],[2.334,0.0],[4.334,0.0],[6.334,0.0],[8.334,0.0],[8.0,2.334],[8.0,4.334],[6.0,4.334],[4.0,4.334],[2.0,4.334],[0.0,4.334],[0.0,1.666]])        \n",
        "        \n",
        "        top_locs = np.empty([pred.shape[0],2])\n",
        "        for k in range(pred.shape[0]):\n",
        "          top_locs[k,:] = np.sum(cord[top_k_idx[k,:],:] * top_k_preds[k,:][:,np.newaxis], axis = 0)\n",
        "\n",
        "        #act_locs = y_test\n",
        "        act_idx = y.argmax(axis = 1)\n",
        "        #act_cords = np.array([[0.334,0.0],[2.334,0.0],[4.334,0.0],[6.334,0.0],[8.0,0.334],[8.0,2.334],[7.666,4.0],[5.666,4.0],[3.666,4.0],[1.666,4.0],[0.0,3.666],[0.0,1.666]])\n",
        "        \n",
        "        act_locs = np.empty([pred.shape[0],2])\n",
        "        act_locs = y_test\n",
        "        \n",
        "        \n",
        "        dists = np.sqrt(np.sum(np.square(top_locs - act_locs),axis = 1))\n",
        "        avg_error = np.mean(dists)\n",
        "        print('Average mse = : {}'.format(avg_error))\n",
        "        \n",
        "        n = top_locs.shape[0]\n",
        "\n",
        "        g1 = (top_locs[0:int(n/5),0], top_locs[0:int(n/5),1]) \n",
        "        g2 = (top_locs[int(n/5):int(2*n/5),0], top_locs[int(n/5):int(2*n/5),1]) \n",
        "        g3 = (top_locs[int(2*n/5):int(3*n/5),0], top_locs[int(2*n/5):int(3*n/5),1]) \n",
        "        g4 = (top_locs[int(3*n/5):int(4*n/5),0], top_locs[int(3*n/5):int(4*n/5),1]) \n",
        "        g5 = (top_locs[int(4*n/5):int(n),0], top_locs[int(4*n/5):int(n),1]) \n",
        "        data = (g1,g2,g3,g4,g5)\n",
        "        colors = ('red','green','blue','brown','black')\n",
        "        for data, color in zip(data, colors):\n",
        "          d1, d2 = data\n",
        "          plt.scatter(np.mean(d1), np.mean(d2), alpha=0.8, c=color, edgecolors='none', s=30)\n",
        "          #plt.scatter(d1, d2, alpha=0.8, c=color, edgecolors='none', s=30)\n",
        "\n",
        "        plt.scatter([0.3,0.9,0.3,0.9,0.3],[0.9,1.5,2.1,2.7,3.3], color = 'purple')\n",
        "\n",
        "        plt.title('Scatter plot')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "#Machine Learning Model for TPU#######################################################################\n",
        "%matplotlib inline\n",
        "print(keras.__version__)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(Dense(64, input_shape =(90,), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "model.add(Dense(15, activation='softmax'))\n",
        "\n",
        "\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate=0.0001)\n",
        "#optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "#Machine Learning Model for TPU#######################################################################\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "model.layers[0].set_weights(autoencoder1.layers[2].get_weights())\n",
        "model.layers[1].set_weights(autoencoder1.layers[3].get_weights())\n",
        "model.layers[2].set_weights(autoencoder2.layers[2].get_weights())\n",
        "model.layers[3].set_weights(autoencoder2.layers[3].get_weights())\n",
        "model.layers[4].set_weights(autoencoder3.layers[2].get_weights())\n",
        "model.layers[5].set_weights(autoencoder3.layers[3].get_weights())\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"./drive/My Drive/Localization/keras_models/24-loc-2/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "    \n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "tpu_model.summary()\n",
        "#mc = keras.callbacks.ModelCheckpoint('./drive/My Drive/Localization/keras_models/5-loc-1/weights{epoch:08d}.h5', \n",
        "#                                     save_weights_only=True, period=50)\n",
        "\n",
        "#tpu_model.reset_states()\n",
        "#tpu_model.load_weights('./drive/My Drive/Localization/keras_models/24-loc-2/model{epoch:08d}.h5')\n",
        "\n",
        "history = tpu_model.fit(x_train, y_train,\n",
        "                          epochs=2000,\n",
        "                          batch_size=1280,\n",
        "                          validation_data  = [x_val, y_val] , shuffle = True, callbacks=[plot, AvgError([x_test1,y_test1])])\n",
        "tpu_model.save('./drive/My Drive/Localization/keras_models/24-loc-2/model{epoch:08d}-1.h5', overwrite=True)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH3Hz1It_KO5"
      },
      "source": [
        "#Machine Learning Model for Regression\n",
        "#Plot Loss\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Flatten, Dense, Activation\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM,Dropout,Conv2D\n",
        "from tensorflow.python.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.layers import BatchNormalization\n",
        "from tensorflow.python.keras.layers import Flatten,MaxPooling2D,AveragePooling2D\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "\n",
        "class PlotLosses(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        \n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.i += 1\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        plt.plot(self.x, self.losses, label=\"loss\")\n",
        "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        plt.legend()\n",
        "        plt.show();\n",
        "        \n",
        "plot_losses = PlotLosses()\n",
        "\n",
        "\n",
        "class PlotLearning(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.acc = []\n",
        "        self.val_acc = []\n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "       \n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.acc.append(logs.get('acc'))\n",
        "        self.val_acc.append(logs.get('val_acc'))\n",
        "        self.i += 1\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        ax1.set_yscale('log')\n",
        "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
        "        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        ax1.legend()\n",
        "        \n",
        "        ax2.plot(self.x, self.acc, label=\"accuracy\")\n",
        "        ax2.plot(self.x, self.val_acc, label=\"validation accuracy\")\n",
        "        ax2.legend()\n",
        "        \n",
        "        plt.show();\n",
        "        \n",
        "plot = PlotLearning()\n",
        "\n",
        "\n",
        "\n",
        "class AvgError(keras.callbacks.Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "      {}\n",
        "      \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        p = 6\n",
        "        x_test, y_test = self.test_data\n",
        "        top_locs = tpu_model.predict(x_test)\n",
        "        act_locs = np.empty([top_locs.shape[0],2])\n",
        "        act_locs = y_test\n",
        "        \n",
        "        sq = np.square(top_locs - act_locs)\n",
        "        avg_error = np.mean(np.sqrt(sq[:,0] + sq[:,1]))\n",
        "        print('Average mse = : {}'.format(avg_error))\n",
        "        \n",
        "        n = top_locs.shape[0]\n",
        "\n",
        "        for i in range(p):\n",
        "          plt.scatter(np.mean(top_locs[int(i * n/p):int((i+1) * n/p),0]), np.mean(top_locs[int(i * n/p):int((i+1) * n/p),1]))\n",
        "          #plt.scatter(top_locs[int(i * n/p):int((i+1) * n/p),0], top_locs[int(i * n/p):int((i+1) * n/p),1])\n",
        "        plt.scatter([1.2,1.8,2.4,1.8],[0.9,1.5,0.9,0.3], color = 'purple')\n",
        "        plt.axis((-1,4,-0.5,2.5))\n",
        "\n",
        "\n",
        "        plt.title('Scatter plot')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def on_train_batch_begin(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "def on_train_batch_end(self, batch, logs=None):\n",
        "      {}\n",
        "    \n",
        "ReduceLROnPlateau.on_train_batch_begin = on_train_batch_begin\n",
        "ReduceLROnPlateau.on_train_batch_end = on_train_batch_end\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
        "                              patience=5, min_lr=0.000001, verbose = 1)\n",
        "\n",
        "\n",
        "#Machine Learning Model for TPU#######################################################################\n",
        "%matplotlib inline\n",
        "print(keras.__version__)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "#model.add(BatchNormalization(input_shape=(90,30,3)))\n",
        "#model.add(Conv2D(10, kernel_size=1, activation='relu'))\n",
        "#model.add(Dropout(0.4))\n",
        "#model.add(AveragePooling2D(pool_size=2))\n",
        "\n",
        "#model.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
        "#model.add(Dropout(0.4))\n",
        "#model.add(AveragePooling2D(pool_size=2))\n",
        "\n",
        "#model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "#model.add(Dropout(0.4))\n",
        "#model.add(AveragePooling2D(pool_size=2))\n",
        "\n",
        "#model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "#model.add(Dropout(0.4))\n",
        "#model.add(AveragePooling2D(pool_size=2))\n",
        "\n",
        "#model.add(Conv2D(256, kernel_size=3, activation='relu', padding = 'same'))\n",
        "#model.add(Dropout(0.4))\n",
        "#model.add(AveragePooling2D(pool_size=2))\n",
        "\n",
        "#model.add(Flatten())\n",
        "\n",
        "model.add(Dense(64, input_shape =(90,), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "model.add(Dense(2, activation='linear'))\n",
        "\n",
        "\n",
        "#optimizer=tf.train.AdamOptimizer(learning_rate=0.01)\n",
        "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
        "\n",
        "#optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "#Machine Learning Model for TPU#######################################################################\n",
        "\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "model.layers[0].set_weights(autoencoder1.layers[2].get_weights())\n",
        "model.layers[1].set_weights(autoencoder1.layers[3].get_weights())\n",
        "model.layers[3].set_weights(autoencoder2.layers[2].get_weights())\n",
        "model.layers[4].set_weights(autoencoder2.layers[3].get_weights())\n",
        "model.layers[6].set_weights(autoencoder3.layers[2].get_weights())\n",
        "model.layers[7].set_weights(autoencoder3.layers[3].get_weights())\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"./drive/My Drive/Localization2/keras_models/15-loc-1/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "    \n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "tpu_model = tf.contrib. tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "tpu_model.summary()\n",
        "#mc = keras.callbacks.ModelCheckpoint('./drive/My Drive/Localization/keras_models/5-loc-1/weights{epoch:08d}.h5', \n",
        "#                                     save_weights_only=True, period=50)\n",
        "\n",
        "#tpu_model.reset_states()\n",
        "#tpu_model.load_weights('./drive/My Drive/Localization/keras_models/24-loc-2/model{epoch:08d}.h5')\n",
        "\n",
        "history = tpu_model.fit(x_train, y_train,\n",
        "                          epochs=1000,\n",
        "                          batch_size=1280,\n",
        "                          validation_data  = [x_val,y_val] , shuffle = True, callbacks=[plot, reduce_lr , AvgError([x_test1,y_test1])])\n",
        "tpu_model.save('./drive/My Drive/Localization/keras_models/24-loc-2/model{epoch:08d}-1.h5', overwrite=True)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar_4uXeppzJC",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c39eb2a-51c8-42bc-aeac-ff56735a489a"
      },
      "source": [
        "#@title\n",
        "#Machine Learning Model for Combining Classification and then Regression\n",
        "#Plot Loss\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Flatten, Dense, Activation\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM,Dropout,Conv2D\n",
        "from tensorflow.python.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.layers import BatchNormalization, Lambda\n",
        "from tensorflow.python.keras.layers import Flatten,MaxPooling2D,AveragePooling2D\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class PlotLosses(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        \n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.i += 1\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        plt.plot(self.x, self.losses, label=\"loss\")\n",
        "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        plt.legend()\n",
        "        plt.show();\n",
        "        \n",
        "plot_losses = PlotLosses()\n",
        "\n",
        "\n",
        "class PlotLearning(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.acc = []\n",
        "        self.val_acc = []\n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "       \n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.acc.append(logs.get('acc'))\n",
        "        self.val_acc.append(logs.get('val_acc'))\n",
        "        self.i += 1\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        ax1.set_yscale('log')\n",
        "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
        "        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        ax1.legend()\n",
        "        \n",
        "        ax2.plot(self.x, self.acc, label=\"accuracy\")\n",
        "        ax2.plot(self.x, self.val_acc, label=\"validation accuracy\")\n",
        "        ax2.legend()\n",
        "        \n",
        "        plt.show();\n",
        "        \n",
        "plot = PlotLearning()\n",
        "\n",
        "\n",
        "class AvgError(keras.callbacks.Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "      {}\n",
        "      \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        p = 6\n",
        "        x_test, y_test = self.test_data\n",
        "        top_locs = tpu_model.predict(x_test)\n",
        "        act_locs = np.empty([top_locs.shape[0],2])\n",
        "        act_locs = y_test\n",
        "        \n",
        "        sq = np.square(top_locs - act_locs)\n",
        "        avg_error = np.mean(np.sqrt(sq[:,0] + sq[:,1]))\n",
        "        print('Average mse = : {}'.format(avg_error))\n",
        "        \n",
        "        n = top_locs.shape[0]\n",
        "\n",
        "        for i in range(p):\n",
        "          plt.scatter(np.mean(top_locs[int(i * n/p):int((i+1) * n/p),0]), np.mean(top_locs[int(i * n/p):int((i+1) * n/p),1]))\n",
        "#        plt.scatter(top_locs[int(i * n/p):int((i+1) * n/p),0], top_locs[int(n/2):,1])\n",
        "        plt.scatter([1.2,1.8,2.4,1.8],[0.9,1.5,0.9,0.3], color = 'purple')\n",
        "        plt.axis((-1,4,-0.5,2.5))\n",
        "\n",
        "\n",
        "        plt.title('Scatter plot')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def on_train_batch_begin(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "def on_train_batch_end(self, batch, logs=None):\n",
        "      {}\n",
        "    \n",
        "ReduceLROnPlateau.on_train_batch_begin = on_train_batch_begin\n",
        "ReduceLROnPlateau.on_train_batch_end = on_train_batch_end\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
        "                              patience=5, min_lr=0.000001, verbose = 1)\n",
        "\n",
        "\n",
        "#Machine Learning Model for TPU#######################################################################\n",
        "%matplotlib inline\n",
        "print(keras.__version__)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(Dense(64, input_shape =(90,), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "#model.add(Dense(16, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "model.add(Dense(15, activation='softmax'))\n",
        "\n",
        "model.add(Lambda(softmax2coord,\n",
        "                 output_shape=softmax2coord_output_shape))\n",
        "\n",
        "#optimizer=tf.train.AdamOptimizer(learning_rate=0.01)\n",
        "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
        "\n",
        "#optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "#Machine Learning Model for TPU#######################################################################\n",
        "\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "#model.layers[0].set_weights(autoencoder1.layers[2].get_weights())\n",
        "#model.layers[1].set_weights(autoencoder1.layers[3].get_weights())\n",
        "#model.layers[3].set_weights(autoencoder2.layers[2].get_weights())\n",
        "#model.layers[4].set_weights(autoencoder2.layers[3].get_weights())\n",
        "#model.layers[6].set_weights(autoencoder3.layers[2].get_weights())\n",
        "#model.layers[7].set_weights(autoencoder3.layers[3].get_weights())\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"./drive/My Drive/Localization2/keras_models/15-loc-1/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "    \n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "tpu_model = tf.contrib. tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "tpu_model.summary()\n",
        "#mc = keras.callbacks.ModelCheckpoint('./drive/My Drive/Localization/keras_models/5-loc-1/weights{epoch:08d}.h5', \n",
        "#                                     save_weights_only=True, period=50)\n",
        "\n",
        "#tpu_model.reset_states()\n",
        "#tpu_model.load_weights('./drive/My Drive/Localization/keras_models/24-loc-2/model{epoch:08d}.h5')\n",
        "\n",
        "history = tpu_model.fit(x_train, y_train,\n",
        "                          epochs=1000,\n",
        "                          batch_size=1280,\n",
        "                          validation_data  = [x_val,y_val] , shuffle = True, callbacks=[plot, reduce_lr , AvgError([x_test1,y_test1])])\n",
        "tpu_model.save('./drive/My Drive/Localization/keras_models/24-loc-2/model{epoch:08d}-1.h5', overwrite=True)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD4CAYAAAATpHZ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnXQTe7Hxb8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd4240e5-1557-47cb-dd4b-911a025edd5e"
      },
      "source": [
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "def softmax2coord(pred):\n",
        "  cord = tf.convert_to_tensor(np.array([[0.0,0.0],[0.0,0.9],[0.0,1.8],[0.9,0.0],[0.9,0.9],[0.9,1.8],[1.8,0.0],[1.8,0.9],[1.8,1.8],[2.7,0.0],[2.7,0.9],[2.7,1.8],\n",
        "                    [3.6,0.0],[3.6,0.9],[3.6,1.8]]) , dtype=np.float32)\n",
        "\n",
        "  t1 = K.dot(pred , K.expand_dims(cord[:,0], axis = 1))\n",
        "  t2 = K.dot(pred , K.expand_dims(cord[:,1], axis = 1))\n",
        "\n",
        "  return K.concatenate([t1,t2], axis = 1)\n",
        "  \n",
        "def softmax2coord_output_shape(input_shape):\n",
        "  shape = list(input_shape)\n",
        "  shape[-1] = 2\n",
        "  return tuple(shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxejDK2XAgsp"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def softmax2coord_tf(pred):\n",
        "  return tf.py_func(softmax2coord, [pred], [tf.float32])\n",
        "  pred = np.array(pred)\n",
        "\n",
        "def softmax2coord(pred):\n",
        "  cord = np.array([[0.0,0.0],[0.0,0.9],[0.0,1.8],[0.9,0.0],[0.9,0.9],[0.9,1.8],[1.8,0.0],[1.8,0.9],[1.8,1.8],[2.7,0.0],[2.7,0.9],[2.7,1.8],\n",
        "                        [3.6,0.0],[3.6,0.9],[3.6,1.8]])\n",
        "  \n",
        "  top_locs = np.empty([pred.shape[0],2])\n",
        "  for k in range(pred.shape[0]):\n",
        "    top_locs[k,:] = np.sum(cord[top_k_idx[k,:],:] * top_k_preds[k,:][:,np.newaxis], axis=0)\n",
        "  return top_locs\n",
        "\n",
        "def softmax2coord_output_shape(input_shape):\n",
        "  shape = list(input_shape)\n",
        "  shape[-1] = 2\n",
        "  return tuple(shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w46sZUCxDRtz"
      },
      "source": [
        "# Defining Models for Classification and Regression (without AutoEncoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqQ-_TBNDbm2"
      },
      "source": [
        "#Machine Learning Model for Classification\n",
        "#Plot Loss\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Activation\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM,Dropout,Conv2D\n",
        "from tensorflow.python.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.layers import BatchNormalization\n",
        "from tensorflow.python.keras.layers import Flatten,MaxPooling2D,AveragePooling2D\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "\n",
        "class PlotLosses(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        \n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.i += 1\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        plt.plot(self.x, self.losses, label=\"loss\")\n",
        "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        plt.legend()\n",
        "        plt.show();\n",
        "        \n",
        "plot_losses = PlotLosses()\n",
        "\n",
        "\n",
        "class PlotLearning(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.acc = []\n",
        "        self.val_acc = []\n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "       \n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.acc.append(logs.get('acc'))\n",
        "        self.val_acc.append(logs.get('val_acc'))\n",
        "        self.i += 1\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        ax1.set_yscale('log')\n",
        "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
        "        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        ax1.legend()\n",
        "        \n",
        "        ax2.plot(self.x, self.acc, label=\"accuracy\")\n",
        "        ax2.plot(self.x, self.val_acc, label=\"validation accuracy\")\n",
        "        ax2.legend()\n",
        "        \n",
        "        plt.show();\n",
        "        \n",
        "plot = PlotLearning()\n",
        "\n",
        "\n",
        "class AvgError(keras.callbacks.Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "      {}\n",
        "      \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        x_test, y_test = self.test_data\n",
        "        #loss, acc = self.model.evaluate(x, y, verbose=0)\n",
        "        k = 15\n",
        "        pred = np.array(self.model.predict(x_test,verbose = 0))\n",
        "        top_k_idx = pred.argsort(axis = 1)[:,-1*k:][:,::-1]\n",
        "        top_k_preds = np.empty([pred.shape[0],k])\n",
        "\n",
        "        for k in range(pred.shape[0]):\n",
        "          top_k_preds[k] = pred[k,:][top_k_idx[k,:]]\n",
        "\n",
        "#        for k in range(pred.shape[0]):\n",
        "#          top_k_preds[k,:] = np.exp(top_k_preds[k,:]) / np.sum(np.exp(top_k_preds[k,:]))\n",
        "\n",
        "        cord = np.array([[0.0,0.0],[0.0,0.9],[0.0,1.8],[0.9,0.0],[0.9,0.9],[0.9,1.8],[1.8,0.0],[1.8,0.9],[1.8,1.8],[2.7,0.0],[2.7,0.9],[2.7,1.8],[3.6,0.0],[3.6,0.9],[3.6,1.8]])\n",
        "        \n",
        "#        cord = np.array([[0.334,0.0],[2.334,0.0],[4.334,0.0],[6.334,0.0],[8.334,0.0],[8.0,2.334],[8.0,4.334],[6.0,4.334],[4.0,4.334],[2.0,4.334],[0.0,4.334],[0.0,1.666]])        \n",
        "        \n",
        "        top_locs = np.empty([pred.shape[0],2])\n",
        "        for k in range(pred.shape[0]):\n",
        "          top_locs[k,:] = np.sum(cord[top_k_idx[k,:],:] * top_k_preds[k,:][:,np.newaxis], axis = 0)\n",
        "\n",
        "        #act_locs = y_test\n",
        "        act_idx = y.argmax(axis = 1)\n",
        "        #act_cords = np.array([[0.334,0.0],[2.334,0.0],[4.334,0.0],[6.334,0.0],[8.0,0.334],[8.0,2.334],[7.666,4.0],[5.666,4.0],[3.666,4.0],[1.666,4.0],[0.0,3.666],[0.0,1.666]])\n",
        "        act_cords = np.array([[0.0,0.0],[0.0,0.6],[0.0,1.2],[0.0,1.8],[0.0,2.4],[0.0,3.0],[0.0,3.6],[0.0,4.2],[0.6,0.0],[0.6,0.6],[0.6,1.2],[0.6,1.8],[0.6,2.4],[0.6,3.0],[0.6,3.6],[0.6,4.2],\n",
        "                        [1.2,0.0],[1.2,0.6],[1.2,1.2],[1.2,1.8],[1.2,2.4],[1.2,3.0],[1.2,3.6],[1.2,4.2]])\n",
        "        act_locs = np.empty([pred.shape[0],2])\n",
        "\n",
        "        for k in range(pred.shape[0]):\n",
        "          act_locs[k,:] = act_cords[act_idx[k],:]\n",
        "        \n",
        "        \n",
        "        dists = np.sqrt(np.sum(np.square(top_locs - act_locs),axis = 1))\n",
        "        avg_error = np.mean(dists)\n",
        "        print('Average mse = : {}'.format(avg_error))\n",
        "        \n",
        "        d1, d2 = top_locs[:,0],top_locs[:,1]\n",
        "        plt.scatter(d1, d2, alpha=0.8, edgecolors='none', s=30)\n",
        "\n",
        "        plt.title('Scatter plot')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "#Machine Learning Model for TPU\n",
        "%matplotlib inline\n",
        "print(keras.__version__)\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(BatchNormalization(input_shape=(30,30,3)))\n",
        "#model.add(Conv2D(10, kernel_size=1, activation='relu',input_shape=(30,30,3)))\n",
        "#model.add(AveragePooling2D(pool_size=2))\n",
        "#model.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
        "#model.add(AveragePooling2D(pool_size=2))\n",
        "#model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "#model.add(AveragePooling2D(pool_size=2, padding = 'same'))\n",
        "#model.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
        "#model.add(AvberagePooling2D(pool_size=2, padding = 'same'))\n",
        "#model.add(Flatten())\n",
        "\n",
        "model.add(Dense(160, input_shape=(90,), activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(150, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(140, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(15, activation='softmax'))\n",
        "\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate=0.0001)\n",
        "#optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"./drive/My Drive/Localization2/keras_models/15-loc-1/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "    \n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "tpu_model.summary()\n",
        "#mc = keras.callbacks.ModelCheckpoint('./drive/My Drive/Localization2/keras_models/5-loc-1/weights{epoch:08d}.h5', \n",
        "#                                     save_weights_only=True, period=50)\n",
        "\n",
        "\n",
        "#tpu_model.load_weights('./drive/My Drive/WifiActivityRecognition/keras_models/8-activity-4/weights00001150.h5')\n",
        "\n",
        "history = tpu_model.fit(x_train, y_train,\n",
        "                          epochs=2000,\n",
        "                          batch_size=1024,\n",
        "                          validation_data  = [x_val,y_val] , shuffle = True, callbacks=[plot, AvgError([x_test1,y_test1])])\n",
        "tpu_model.save('./drive/My Drive/Localization/keras_models/24-loc-1/model{epoch:08d}.h5', overwrite=True)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x-LqkIsKV4w"
      },
      "source": [
        "#Machine Learning Model for Regression\n",
        "#Plot Loss\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Flatten, Dense, Activation\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM,Dropout,Conv2D\n",
        "from tensorflow.python.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.layers import BatchNormalization\n",
        "from tensorflow.python.keras.layers import Flatten,MaxPooling2D,AveragePooling2D\n",
        "from keras import metrics\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "\n",
        "class PlotLosses(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        \n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.i += 1\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        plt.plot(self.x, self.losses, label=\"loss\")\n",
        "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        plt.legend()\n",
        "        plt.show();\n",
        "        \n",
        "plot_losses = PlotLosses()\n",
        "\n",
        "\n",
        "class PlotLearning(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.acc = []\n",
        "        self.val_acc = []\n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "       \n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.acc.append(logs.get('acc'))\n",
        "        self.val_acc.append(logs.get('val_acc'))\n",
        "        self.i += 1\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        ax1.set_yscale('log')\n",
        "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
        "        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        ax1.legend()\n",
        "        \n",
        "        ax2.plot(self.x, self.acc, label=\"accuracy\")\n",
        "        ax2.plot(self.x, self.val_acc, label=\"validation accuracy\")\n",
        "        ax2.legend()\n",
        "        \n",
        "        plt.show();\n",
        "        \n",
        "plot = PlotLearning()\n",
        "\n",
        "\n",
        "\n",
        "class AvgError(keras.callbacks.Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "      {}\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "      {}\n",
        "      \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        x_test, y_test = self.test_data\n",
        "        top_locs = tpu_model.predict(x_test)\n",
        "        act_locs = np.empty([top_locs.shape[0],2])\n",
        "        act_locs = y_test\n",
        "        \n",
        "        dists = np.sqrt(np.sum(np.square(top_locs - act_locs),axis = 1))\n",
        "        avg_error = np.mean(dists)\n",
        "        print('Average mae = : {}'.format(avg_error))\n",
        "        \n",
        "        n = top_locs.shape[0]\n",
        "\n",
        "        plt.scatter(top_locs[:,0],top_locs[:,1])\n",
        "\n",
        "        plt.title('Scatter plot')\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "#Machine Learning Model for TPU#######################################################################\n",
        "%matplotlib inline\n",
        "print(keras.__version__)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(BatchNormalization(input_shape=(25,30,3)))\n",
        "model.add(Conv2D(10, kernel_size=1, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "#model.add(AveragePooling2D(pool_size=2))\n",
        "\n",
        "model.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "#model.add(AveragePooling2D(pool_size=2))\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "#model.add(AveragePooling2D(pool_size=2))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "#model.add(AveragePooling2D(pool_size=2))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=3, activation='relu', padding = 'same'))\n",
        "model.add(Dropout(0.4))\n",
        "#model.add(AveragePooling2D(pool_size=2))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(512, input_shape =(90,), activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "#model.add(Dense(150, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.3))\n",
        "\n",
        "#model.add(Dense(140, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(2, activation='linear'))\n",
        "\n",
        "\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "#optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "#Machine Learning Model for TPU#######################################################################\n",
        "\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"./drive/My Drive/Localization2/keras_models/15-loc-1/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "    \n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "tpu_model.summary()\n",
        "#mc = keras.callbacks.ModelCheckpoint('./drive/My Drive/Localization/keras_models/5-loc-1/weights{epoch:08d}.h5', \n",
        "#                                     save_weights_only=True, period=50)\n",
        "\n",
        "#tpu_model.reset_states()\n",
        "#tpu_model.load_weights('./drive/My Drive/Localization/keras_models/24-loc-2/model{epoch:08d}.h5')\n",
        "\n",
        "history = tpu_model.fit(x_train, y_train,\n",
        "                          epochs=2000,\n",
        "                          batch_size=1280,\n",
        "                          validation_data  = [x_val,y_val] , shuffle = True, callbacks=[plot, AvgError([x_test1,y_test1])])\n",
        "tpu_model.save('./drive/My Drive/Localization/keras_models/24-loc-2/model{epoch:08d}-1.h5', overwrite=True)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up7sKs8D_MB_"
      },
      "source": [
        "# Extra Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c30XWS3mAYZs"
      },
      "source": [
        "#Convert Data into Images for CNN\n",
        "def data_convert(x,y):\n",
        "  t = 25\n",
        "  x_conv = np.empty([0,t,30,3])\n",
        "  #tmp = np.empty([0,t,30,3])\n",
        "  n = x.shape[0]\n",
        "  n = n - n%t\n",
        " \n",
        "  for k in range(int(n/t)):\n",
        "    r = x[int(k*t):int((k+1)*t),0:30]\n",
        "    g = x[int(k*t):int((k+1)*t),30:60]\n",
        "    b = x[int(k*t):int((k+1)*t),60:90]\n",
        "\n",
        "    tmp = np.dstack((r,g,b))\n",
        "    tmp = np.expand_dims(tmp, axis = 0)\n",
        "    x_conv = np.concatenate((x_conv,tmp), axis = 0)\n",
        "  y_conv = y[::t,:]\n",
        "  \n",
        "  if y_conv.shape[0] != x_conv.shape[0]:\n",
        "    y_conv = y_conv[:x_conv.shape[0],:]\n",
        "  print('Done')\n",
        "  return x_conv,y_conv\n",
        "\n",
        "\n",
        "x_train,y_train = data_convert(x_train,y_train)\n",
        "x_val,y_val = data_convert(x_val,y_val)\n",
        "x_test,y_test = data_convert(x_test,y_test)\n",
        "x_test1, y_test1 = data_convert(x_test1,y_test1)\n",
        "\n",
        "n = x_test1.shape[0]\n",
        "n = n - n%8\n",
        "x_test1 = x_test1[:n,:]\n",
        "y_test1 = y_test1[:n,:]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "print(x_test1.shape)\n",
        "print(y_test1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqHKloskvV_e"
      },
      "source": [
        "#Convert data into collection of r samples\n",
        "def data_convert(x,y):\n",
        "  r = 90\n",
        "  x_conv = np.empty([0,r,90])\n",
        "  n = x.shape[0]\n",
        "  n = n - n%r\n",
        "  for k in range(int(n/r)):\n",
        "    \n",
        "    tmp = x[k*r:(k+1)*r,:]\n",
        "    tmp = np.expand_dims(tmp, axis=0)\n",
        "    x_conv = np.concatenate((x_conv,tmp),axis = 0)\n",
        "    y_conv = y[::r,:]\n",
        "  return x_conv,y_conv\n",
        "\n",
        "\n",
        "x,y = data_convert(x,y)\n",
        "x_test1, y_test1 = data_convert(x_test1,y_test1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZbZwe8jvmgr"
      },
      "source": [
        "#Reduce Dimentionality using AutoEncoder\n",
        "from IPython.display import Image, SVG\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
        "from keras import regularizers\n",
        "\n",
        "input_dim = x_train.shape[1]\n",
        "encoding_dim = 5\n",
        "\n",
        "autoencoder = Sequential()\n",
        "autoencoder.add(Dense(50, input_shape=(input_dim,), activation='relu'))\n",
        "autoencoder.add(Dense(30, input_shape=(input_dim,), activation='relu'))\n",
        "autoencoder.add(Dense(10, input_shape=(input_dim,), activation='relu'))\n",
        "\n",
        "autoencoder.add(Dense(encoding_dim, activation='relu'))\n",
        "\n",
        "autoencoder.add(Dense(10, activation='relu'))\n",
        "autoencoder.add(Dense(30, input_shape=(input_dim,), activation='relu'))\n",
        "autoencoder.add(Dense(50, input_shape=(input_dim,), activation='relu'))\n",
        "\n",
        "autoencoder.add(Dense(input_dim, activation='sigmoid'))\n",
        "autoencoder.summary()\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "\n",
        "autoencoder.compile(optimizer=optimizer, loss='mse')\n",
        "autoencoder.load_weights('./drive/My Drive/Localization/keras_models/autoencoderlinear100hz.h5')\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=1000,\n",
        "                batch_size=1280,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))\n",
        "\n",
        "autoencoder.save('./drive/My Drive/Localization/keras_models/autoencoderlinear100hz-1.h5')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTR8RLBPvwu_"
      },
      "source": [
        "from keras.models import load_model\n",
        "autoencoder = load_model('./drive/My Drive/Localization/keras_models/autoencoder20hz-1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Su911SMvzcY"
      },
      "source": [
        "from IPython.display import Image, SVG\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "input_dim = 90\n",
        "input_img = Input(shape=(input_dim,))\n",
        "encoder_layer1 = autoencoder.layers[0]\n",
        "encoder_layer2 = autoencoder.layers[1]\n",
        "encoder_layer3 = autoencoder.layers[2]\n",
        "encoder_layer4 = autoencoder.layers[3]\n",
        "encoder = Model(input_img, encoder_layer4(encoder_layer3(encoder_layer2(encoder_layer1(input_img)))))\n",
        "\n",
        "encoded_x = encoder.predict(x)\n",
        "encoded_x_test1 = encoder.predict(x_test1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwU-4T4936nD"
      },
      "source": [
        "#Use T-SNE on data to reduce dimension for visualization\n",
        "import time\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "n_sne = .9216\n",
        "\n",
        "time_start = time.time()\n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=500)\n",
        "tsne_results = tsne.fit_transform(x)\n",
        "\n",
        "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1w_PHKR38eK"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "i = 4\n",
        "n = 384\n",
        "plt.scatter(tsne_results[i*n:i*n + n,0], tsne_results[i*n:i*n + n,1])\n",
        "plt.title('Scatter plot')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsTQldUzDkjn"
      },
      "source": [
        "#Print Confusion Matrix\n",
        "from mlxtend.evaluate import confusion_matrix\n",
        "\n",
        "pred = tpu_model.predict(x)\n",
        "cm = confusion_matrix(y_target=np.argmax(y,1),y_predicted=np.argmax(pred,1),binary=False)\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IySiMRu6XJN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "321f0530-76d7-491f-b3ea-a1448652114a"
      },
      "source": [
        "#Import Waqai Test Data\n",
        "#Importing Testing Data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import csv\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "ds = 50 #Downsampling Factor\n",
        "\n",
        "x_test2 = np.empty(shape = [0,90], dtype = float)\n",
        "y_test2 = np.empty(shape = [0,2], dtype = float)\n",
        "\n",
        "count = 0\n",
        "\n",
        "for i in sorted(glob.glob(\"./drive/My Drive/Localization2/Dataset/Test2/*\")):\n",
        "  print(\"input_file_name=\",i )\n",
        "  data = [[ float(elm) for elm in v] for v in csv.reader(open(i, \"r\"))]\n",
        "  tmp1 = np.array(data)\n",
        "  tmp1 = tmp1[:59200,1:91]\n",
        "  tmp2 = tmp1[::ds,:]\n",
        "  #r,c = np.shape(tmp1)\n",
        "  x_test2 = np.concatenate((x_test2,tmp2),axis = 0)\n",
        "  r = tmp2.shape[0]\n",
        "  yy = np.empty([r,2],float)\n",
        "  \n",
        "  \n",
        "  if count == 0 :\n",
        "     yy[:,:] = np.array([0.6,0.0]) \n",
        "  elif count == 1:\n",
        "     yy[:,:] = np.array([1.2,0.0])\n",
        "  elif count == 2:\n",
        "     yy[:,:] = np.array([0.0,0.0])\n",
        "      \n",
        "  y_test2 = np.concatenate((y_test2,yy),axis = 0)\n",
        "  \n",
        "  count = count + 1\n",
        "  \n",
        "  \n",
        "  \n",
        "  print(np.shape(x_test2))\n",
        "  print(np.shape(y_test2))\n",
        "  print(str(i) + \" -------->> done\")\n",
        "  \n",
        "#Normalizing Data from 0 to 1 range\n",
        "x_test2 = scaler.transform(x_test2)\n",
        "x_test1 = np.concatenate((x_test1,x_test2),axis = 0)\n",
        "y_test1 = np.concatenate((y_test1,y_test2),axis = 0) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Test2/test1.csv\n",
            "(1184, 90)\n",
            "(1184, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Test2/test1.csv -------->> done\n",
            "input_file_name= ./drive/My Drive/Localization2/Dataset/Test2/test2.csv\n",
            "(2368, 90)\n",
            "(2368, 2)\n",
            "./drive/My Drive/Localization2/Dataset/Test2/test2.csv -------->> done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr_4kAdu8Kjd"
      },
      "source": [
        "#Testing Model on Waqai Test Data\n",
        "p = 2\n",
        "top_locs = tpu_model.predict(x_test2)\n",
        "act_locs = np.empty([top_locs.shape[0],2])\n",
        "act_locs = y_test2\n",
        "\n",
        "dists = np.sqrt(np.sum(np.square(top_locs - act_locs),axis = 1))\n",
        "avg_error = np.mean(dists)\n",
        "print('Average mse = : {}'.format(avg_error))\n",
        "\n",
        "n = top_locs.shape[0]\n",
        "\n",
        "for i in range(p):\n",
        "  plt.scatter(np.mean(top_locs[int(i * n/p):int((i+1) * n/p),0]), np.mean(top_locs[int(n/2):,1]))\n",
        "\n",
        "plt.scatter([0.6,1.2],[0.0,0.0], color = 'purple')\n",
        "plt.axis((-1,4,-0.5,2.5))\n",
        "\n",
        "\n",
        "plt.title('Scatter plot')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_4GRia9XafR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}